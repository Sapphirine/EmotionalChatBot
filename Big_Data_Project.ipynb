{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "############################### Final Design #############################################\n",
    "## Final design with:\n",
    "## Input: Word2vec vectors\n",
    "## Output: One hot matrix\n",
    "## Emotions: Positive 0, Neutral 1 and Negative 2\n",
    "############################### start of code ############################################\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed, Merge\n",
    "#from keras.utils.visualize_util import plot, to_graph\n",
    "#import theano.d3viz as d3v\n",
    "import word2vec_utils as w2v\n",
    "import data\n",
    "import numpy as np\n",
    "from data_utils import split_dataset \n",
    "\n",
    "## Size of data\n",
    "embed_dim = 300\n",
    "data_dim = 1\n",
    "timesteps = 26\n",
    "vocab_size = 8100\n",
    "emot_size = 4\n",
    "max_sent_length = timesteps + emot_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Model\n",
    "w2v_model = w2v.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert input to word to vec matrix\n",
    "metadata, filtered_A1, filtered_B, filtered_A2 = data.load_data()\n",
    "# convert list of [lines of text] into list of [list of words ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm_data = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> w2v our inputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.py:579: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  return word in self.vocab\n"
     ]
    }
   ],
   "source": [
    "print('>> w2v our inputs')\n",
    "A1_w2v = np.array( [ w2v.vectorize( w2v_model, line, pad_length = max_sent_length ) for line in filtered_A1[sm_data:sm_data*2] ] ) \n",
    "B_w2v  = np.array( [ w2v.vectorize( w2v_model, line, pad_length = max_sent_length ) for line in filtered_B[sm_data:sm_data*2]  ] ) \n",
    "A2_w2v = np.array( [ w2v.vectorize( w2v_model, line, pad_length = max_sent_length ) for line in filtered_A2[sm_data:sm_data*2] ] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A1_train, A1_test, A1_val = split_dataset(A1_w2v)\n",
    "B_train,  B_test,  B_val  = split_dataset(B_w2v )\n",
    "A2_train, A2_test, A2_val = split_dataset(A2_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.SGD at 0x7f83629d3650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Neural network model\n",
    "\n",
    "keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Emotional model\n",
    "EM_Model = '''\n",
    "model_Em = Sequential()\n",
    "model_Em.add(LSTM(embed_dim,input_shape=(timesteps,embed_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model_Em.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model_Em.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model_Em.add(Dense(emot_size, activation='softmax'))\n",
    "\n",
    "model_Em.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "model_Em.fit(AB_train,AB_emtrain,\n",
    "          batch_size=128, nb_epoch=20,\n",
    "          validation_data=(AB_val,AB_emval))\n",
    "model.save_weights('model_Em_weights.h5')\n",
    "Em.predict(AB_train).concatenate(AB_train,axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, recurrent_dropout=0.2, dropout=0.2, input_shape=(30, 300), return_sequences=True)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "## model_A takes A's word2vec matrix as input \n",
    "model_A = Sequential()\n",
    "model_A.add(LSTM(embed_dim,input_shape=(max_sent_length,embed_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "\n",
    "model_A.compile(loss='cosine_proximity',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "#model_A.load_weights('model_A_weights.h5')\n",
    "#model_A.fit(A1_train, A2_train,\n",
    "#          batch_size=128, nb_epoch=5,\n",
    "#          validation_data=(A1_val, A2_val))\n",
    "#model_A.save_weights('model_A_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, recurrent_dropout=0.2, dropout=0.2, input_shape=(30, 300), return_sequences=True)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "## model_B takes B's word2vec matrix as input \n",
    "model_B = Sequential()\n",
    "model_B.add(LSTM(embed_dim,input_shape=(max_sent_length,embed_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "\n",
    "model_B.compile(loss='cosine_proximity',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "#model_B.load_weights('model_B_weights.h5')\n",
    "#model_B.fit(B_train, A2_train,\n",
    "#          batch_size=128, nb_epoch=5,\n",
    "#          validation_data=(B_val, A2_val))\n",
    "#model_B.save_weights('model_B_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  app.launch_new_instance()\n",
      "/Users/anthony/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "/Users/anthony/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "## model_ABA concatenates both A and B input and predicts next A\n",
    "model_ABA = Sequential()\n",
    "model_ABA.add(Merge([model_A, model_B], mode = 'concat'))\n",
    "model_ABA.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model_ABA.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "#model_ABA.add(TimeDistributed(Dense(vocab_size,activation='softmax')))\n",
    "\n",
    "model_ABA.compile(loss='cosine_proximity',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "model_ABA.load_weights('model_ABA_weights.h5')\n",
    "#graph = to_graph(model_ABA, show_shape=True)\n",
    "#graph.write_png(\"model.png\")\n",
    "#d3v.d3viz(model_ABA.get_output(), 'test.html')\n",
    "#model_ABA.fit([A1_w2v,B_w2v], A2_w2v,\n",
    "#          batch_size=128, nb_epoch=50,\n",
    "#          validation_split = 0.05 )\n",
    "#          #validation_data=([A1_,B_val],A2_val))\n",
    "#model_ABA.save_weights('model_ABA_weights.h5')\n",
    "\n",
    "#score = model_ABA.evaluate([A1_test,B_test], A2_test, batch_size=64)\n",
    "\n",
    "#print(\"\\nModel Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosign_similarity( a, b):\n",
    "    dot = np.dot(a,b)\n",
    "    norm = np.linalg.norm(a-b)\n",
    "    return( dot/norm )\n",
    "\n",
    "def most_likely_word( prediction, words, w2v_model ):\n",
    "    ret = \"\"\n",
    "    max_so_far = cosign_similarity( np.array([1]+[0]*299),prediction ) #must be better than blank\n",
    "    #Best word in dictionary\n",
    "    for word in words:\n",
    "        similarity = cosign_similarity( w2v.vectorize(w2v_model,word)[0], prediction )\n",
    "        if similarity > max_so_far:\n",
    "            max_so_far = similarity\n",
    "            ret = word    \n",
    "    return( ret )\n",
    "\n",
    "def most_likely_sent( prediction_vec, words, w2v_model ):\n",
    "    predicted_words = []\n",
    "    for prediction in prediction_vec:\n",
    "        predicted_words.append( most_likely_word(prediction,words,w2v_model))\n",
    "    ret = \" \".join(predicted_words)\n",
    "    return( ret )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 9 1 @ yeah do you do just just                    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User_A1 = \"5 5 1 @I really love you. I think I want to marry you!\"\n",
    "User_B = \"1 5 5 @yeah well I hate you and never want to see you again! \"\n",
    "User_A1 = w2v.vectorize( w2v_model, User_A1 , pad_length = max_sent_length) \n",
    "User_B  = w2v.vectorize( w2v_model, User_B, pad_length = max_sent_length) \n",
    "user_input = [ np.array( [User_A1]), np.array([User_B]) ]\n",
    "prediction = model_ABA.predict(user_input)\n",
    "most_likely_sent( prediction[0], metadata['w2idx'].keys(),w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.49282199122841"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosign_similarity( prediction[0][-1], np.array([1]+[0]*299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthony/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 9 1 @ forget it .                       '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_sent(A2_train[0],metadata['w2idx'].keys(),w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 9 1 @ yeah guess you anyway just                     '"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model_ABA.predict([np.array([A1_train[0]]),np.array([B_train[0]])])\n",
    "most_likely_sent(prediction[0],metadata['w2idx'].keys(),w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-afa99aa1cb2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[1;31m#model.add(TimeDistributed(Dense(1,activation='relu')))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m model.compile(loss='mse',\n\u001b[1;32m     38\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "############################### 1) With Embedding ########################################\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed\n",
    "\n",
    "import numpy as np\n",
    "embed_dim = 128\n",
    "data_dim = 1\n",
    "timesteps = 25\n",
    "vocab_size = 8100\n",
    "\n",
    "x_old = np.load('idx_q.npy')\n",
    "y_old = np.load('idx_a.npy')\n",
    "\n",
    "keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "embedding = Sequential()\n",
    "embedding.add(Embedding(vocab_size,embed_dim,input_length=timesteps))\n",
    "embedding.compile('rmsprop', 'mse')\n",
    "\n",
    "x = embedding.predict(x_old)\n",
    "y = embedding.predict(y_old)\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "#model.add(Embedding(8100,embed_dim,input_length=timesteps))\n",
    "model.add(LSTM(embed_dim,input_shape=(timesteps,embed_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "#model.add(TimeDistributed(Dense(1,activation='relu')))\n",
    "\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "# generate dummy training data\n",
    "\n",
    "\n",
    "x_train = x[1:140001].reshape(140000,timesteps,embed_dim)\n",
    "y_train = y[1:140001].reshape(140000,timesteps,embed_dim)\n",
    "# generate dummy validation data\n",
    "x_val = x[140001:150000].reshape(9999,timesteps,embed_dim)\n",
    "y_val = y[140001:150000].reshape(9999,timesteps,embed_dim)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128, nb_epoch=20,\n",
    "          validation_data=(x_val, y_val))\n",
    "model.save('my_model.h5')\n",
    "x_test = x[150001:160000].reshape(9999,timesteps,embed_dim)\n",
    "y_test = y[150001:160000].reshape(9999,timesteps,embed_dim)\n",
    "score = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print(\"\\nModel Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[480337.9356212425, 0.012264528604810367]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=model.predict(x[40001:40500].reshape(499,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09884203  0.08344298  0.09658517  0.09359372  0.08724034  0.07723846\n",
      "  0.0680349   0.06117992  0.05216976  0.04656862  0.03937593  0.03422173\n",
      "  0.03021572  0.02661941  0.02194737  0.01883822  0.01598314  0.0128208\n",
      "  0.01082024  0.00868252  0.00629053  0.00474371  0.00292437  0.00145011\n",
      "  0.00017013]\n"
     ]
    }
   ],
   "source": [
    "print(c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140000 samples, validate on 9999 samples\n",
      "Epoch 1/20\n",
      "140000/140000 [==============================] - 186s - loss: 402546.0464 - acc: 0.1487 - val_loss: 432623.3359 - val_acc: 0.0098\n",
      "Epoch 2/20\n",
      "140000/140000 [==============================] - 177s - loss: 402465.1430 - acc: 0.0107 - val_loss: 432610.5873 - val_acc: 0.0110\n",
      "Epoch 3/20\n",
      " 14208/140000 [==>...........................] - ETA: 149s - loss: 399385.0842 - acc: 0.0108"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "############################ 2) Normal LSTM No Embedding #################################\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed\n",
    "\n",
    "import numpy as np\n",
    "embed_dim = 128\n",
    "data_dim = 1\n",
    "timesteps = 25\n",
    "vocab_size = 8100\n",
    "\n",
    "x = np.load('idx_q.npy')\n",
    "y = np.load('idx_a.npy')\n",
    "\n",
    "keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "#embedding = Sequential()\n",
    "#embedding.add(Embedding(vocab_size,embed_dim,input_length=timesteps))\n",
    "#embedding.compile('rmsprop', 'mse')\n",
    "\n",
    "#x = embedding.predict(x_old)\n",
    "#y = embedding.predict(y_old)\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(data_dim,input_shape=(timesteps,data_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(data_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(data_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "#model.add(TimeDistributed(Dense(embed_dim,activation='relu')))\n",
    "#model.add(TimeDistributed(Dense(data_dim,activation='relu')))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "# generate dummy training data\n",
    "\n",
    "\n",
    "x_train = x[1:140001].reshape(140000,timesteps,data_dim)\n",
    "y_train = y[1:140001].reshape(140000,timesteps,data_dim)\n",
    "# generate dummy validation data\n",
    "x_val = x[140001:150000].reshape(9999,timesteps,data_dim)\n",
    "y_val = y[140001:150000].reshape(9999,timesteps,data_dim)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128, nb_epoch=20,\n",
    "          validation_data=(x_val, y_val))\n",
    "model.save('my_model.h5')\n",
    "x_test = x[150001:160000].reshape(9999,timesteps,data_dim)\n",
    "y_test = y[150001:160000].reshape(9999,timesteps,data_dim)\n",
    "score = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print(\"\\nModel Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140000 samples, validate on 9999 samples\n",
      "Epoch 1/20\n",
      " 70272/140000 [==============>...............] - ETA: 141s - loss: 389574.3568 - acc: 0.0036"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "################################# 3) LSTM One hot ########################################\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed\n",
    "\n",
    "import numpy as np\n",
    "embed_dim = 128\n",
    "data_dim = 1\n",
    "timesteps = 25\n",
    "\n",
    "\n",
    "x = np.load('idx_q.npy')\n",
    "y_old = np.load('idx_a.npy')\n",
    "vocab_size = np.amax(y_old)+1\n",
    "y=(np.arange(y_old.max()) == y_old[:,:,None]-1).astype(int)\n",
    "keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "#embedding = Sequential()\n",
    "#embedding.add(Embedding(vocab_size,embed_dim,input_length=timesteps))\n",
    "#embedding.compile('rmsprop', 'mse')\n",
    "\n",
    "#x = embedding.predict(x_old)\n",
    "#y = embedding.predict(y_old)\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(data_dim,input_shape=(timesteps,data_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(data_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(data_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(TimeDistributed(Dense(vocab_size,activation='softmax')))\n",
    "#model.add(TimeDistributed(Dense(data_dim,activation='relu')))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "# generate dummy training data\n",
    "\n",
    "\n",
    "x_train = x[1:140001].reshape(140000,timesteps,data_dim)\n",
    "y_train = y[1:140001].reshape(140000,timesteps,vocab_size)\n",
    "# generate dummy validation data\n",
    "x_val = x[140001:150000].reshape(9999,timesteps,data_dim)\n",
    "y_val = y[140001:150000].reshape(9999,timesteps,vocab_size)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128, nb_epoch=20,\n",
    "          validation_data=(x_val, y_val))\n",
    "model.save('my_model.h5')\n",
    "x_test = x[150001:160000].reshape(9999,timesteps,data_dim)\n",
    "y_test = y[150001:160000].reshape(9999,timesteps,vocab_size)\n",
    "score = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print(\"\\nModel Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "################################# 4) LSTM Time distributed dense #########################\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed\n",
    "\n",
    "import numpy as np\n",
    "embed_dim = 128\n",
    "data_dim = 1\n",
    "timesteps = 25\n",
    "\n",
    "keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "#model.add(Embedding(8100,embed_dim,input_length=timesteps))\n",
    "model.add(LSTM(data_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(data_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "#model.add(LSTM(data_dim, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(TimeDistributed(Dense(data_dim,activation='relu')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "# generate dummy training data\n",
    "x = np.load('idx_q.npy')\n",
    "y = np.load('idx_a.npy')\n",
    "\n",
    "x_train = x[1:140001].reshape(140000,25)\n",
    "y_train = y[1:140001].reshape(140000,25,1)\n",
    "# generate dummy validation data\n",
    "x_val = x[140001:150000].reshape(9999,25)\n",
    "y_val = y[140001:150000].reshape(9999,25,1)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128, nb_epoch=20,\n",
    "          validation_data=(x_val, y_val))\n",
    "model.save('my_model.h5')\n",
    "x_test = x[150001:160000].reshape(9999,25)\n",
    "y_test = y[150001:160000].reshape(9999,25,1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print(\"\\nModel Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = model.predict(X_test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29208347]\n"
     ]
    }
   ],
   "source": [
    "print(c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166559, 25)\n",
      "8001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "x = np.load('idx_q.npy')\n",
    "y = np.load('idx_a.npy')\n",
    "print(x.shape)\n",
    "a=np.array([[1, 7, 5, 3], # Modified the sample listed in question for variety\n",
    "       [2, 4, 1, 4]])\n",
    "\n",
    "#b=(np.arange(y.max()) == y[:,:,None]-1).astype(int)\n",
    "#b=np.eye(8100)[y]\n",
    "print(np.amax(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
