{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Loaded word_frequencies data from disk\n",
      "1 -1\n",
      "(1, 30, 10003)\n",
      "poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom\n",
      "1 -2\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 11s - loss: -1.1921e-07 - acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s - loss: -1.1921e-07 - acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s - loss: -1.1921e-07 - acc: 0.0000e+00\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "(1, 30, 10003)\n",
      "poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom\n"
     ]
    }
   ],
   "source": [
    "#KERAS_BACKEND=\"theano\"\n",
    "import numpy as np\n",
    "import word2vec_utils as w2v\n",
    "import data\n",
    "from data_utils import split_dataset \n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "import subprocess\n",
    "import shlex\n",
    "import tkinter as tk\n",
    "try:\n",
    "    import ttk as ttk\n",
    "    import ScrolledText\n",
    "except ImportError:\n",
    "    import tkinter.ttk as ttk\n",
    "    import tkinter.scrolledtext as ScrolledText\n",
    "import time\n",
    "class neural_chatbot():\n",
    "    \n",
    "    def __init__(self):\n",
    "        #self.initialize()\n",
    "        self.load_model()\n",
    "        self.w2v_model = w2v.initialize()\n",
    "        self.count=0\n",
    "        self.out='1 9 1 @Hi'\n",
    "        print( 'Loaded word_frequencies data from disk' )\n",
    "        self.word_freqs = np.load('words_in_order_of_freq.npy') \n",
    "        self.embed_dim = 101\n",
    "        self.data_dim = 1\n",
    "        self.timesteps = 26\n",
    "        self.vocab_size = 10000\n",
    "        self.emot_size = 4\n",
    "        self.max_sent_length = 30\n",
    "        self.reinforce = 'neutral'\n",
    "    def save_model(self):\n",
    "        # serialize model to JSON\n",
    "        model_json = self.one_hot_chat_model.to_json()\n",
    "        with open(\"one_hot_chat_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        self.one_hot_chat_model.save_weights(\"one_hot_chat_net.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "    \n",
    "    def load_model(self):\n",
    "        # load json and create model\n",
    "        json_file = open('one_hot_chat_model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        self.one_hot_chat_model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        self.one_hot_chat_model.load_weights(\"one_hot_chat_net.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "    \n",
    "    def RateSentiment(self,sentiString):\n",
    "        #open a subprocess using shlex to get the command line string into the correct args list format\n",
    "        p = subprocess.Popen(shlex.split(\"C:/ProgramData/Oracle/java/javapath/java -jar C:/Users/anul/Downloads/SentiStrength.jar stdin sentidata C:/Users/anul/Downloads/SentStrength_Data_Sept2011/\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE,universal_newlines=True)\n",
    "        #communicate via stdin the string to be rated. Note that all spaces are replaced with +\n",
    "        stdout_text, stderr_text = p.communicate(sentiString.replace(\" \",\"+\"))\n",
    "        #remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1-5\n",
    "        stdout_text = stdout_text.rstrip().replace(\"\\t\",\"\")\n",
    "        return stdout_text   \n",
    "    \n",
    "            \n",
    "    def reinforcement_learn(self,user_emotion,sys_response):\n",
    "        total_emo = int(user_emotion[0]) - int(user_emotion[3])\n",
    "        sys_res = np.argmax(sys_response,axis=2)\n",
    "        adam = keras.optimizers.Adam(lr = 100)#default 0.001\n",
    "        self.one_hot_chat_model.compile( optimizer=adam,loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "        if total_emo > 0:\n",
    "            #sys_res = (10*sys_res)+10\n",
    "            sys_res_categorical=to_categorical(sys_res.astype(int),self.vocab_size+3).reshape((1,self.max_sent_length,self.vocab_size+3))\n",
    "            self.reinforce='positive'\n",
    "            self.one_hot_chat_model.fit(self.user_input,sys_res_categorical,epochs=3)\n",
    "        else:\n",
    "            sys_res_categorical=to_categorical(sys_res.astype(int),self.vocab_size+3).reshape((1,self.max_sent_length,self.vocab_size+3))*(-1)\n",
    "            self.reinforce='negative'\n",
    "            self.one_hot_chat_model.fit(self.user_input,sys_res_categorical,epochs=3)\n",
    "        self.save_model()\n",
    "        self.load_model()\n",
    "        \n",
    "    #def convert3d_to_4d(sys_response3d):\n",
    "    #    sys_res3d_standardized = (10*sys_res)+10\n",
    "    #    sys_response = to_categorical(sys_res3d_standardized.astype(int),20).reshape((1,300,-1,20))\n",
    "\n",
    "    def model_response(self,User_B):\n",
    "        #User_A1 = \"5 5 1 @I really love you. I think I want to marry you!\"\n",
    "        #User_B = \"1 5 5 @yeah well I hate you and never want to see you again! \"\n",
    "        #User_A1 = w2v.vectorize( w2v_model, User_A1 , pad_length = max_sent_length)\n",
    "        Sen_User_B = self.RateSentiment(User_B)\n",
    "        print(Sen_User_B)\n",
    "        self.count=self.count+1\n",
    "        if self.count>1:\n",
    "            self.reinforcement_learn(Sen_User_B,self.sys_prediction)\n",
    "        \n",
    "        User_B = Sen_User_B[0]+\" \"+str(11-int(Sen_User_B[0])-int(Sen_User_B[3]))+\" \"+Sen_User_B[3]+\" \"+\"@\"+User_B\n",
    "        self.User_B_in  = w2v.vectorize( User_B, pad_length = 30,model=self.w2v_model ) \n",
    "        User_A = w2v.vectorize( self.out, pad_length = 30,model=self.w2v_model ) \n",
    "        self.user_input = [ np.array( [User_A]).reshape(1,30,101), np.array([self.User_B_in]).reshape(1,30,101) ]\n",
    "        #print(self.sys_prediction.shape)\n",
    "        self.sys_prediction = self.one_hot_chat_model.predict(self.user_input)\n",
    "        print(self.sys_prediction.shape)\n",
    "        sentence=np.argmax(self.sys_prediction,axis=2)\n",
    "        #w2v.one_hot_unvectorize(np.argmax(self.sys_prediction,axis=2))\n",
    "        pred_words=[]\n",
    "        for i in range(30):\n",
    "            pred_words.append(self.word_freqs[sentence[0,i]].decode('utf-8'))\n",
    "        self.out=\" \".join(pred_words)\n",
    "        print(self.out)    \n",
    "        return (self.out)\n",
    "        #sys_pred = np.argmax(prediction, axis=2)\n",
    "        #self.sys_prediction = (sys_pred-10)/10\n",
    "        #return self.most_likely_sent( self.sys_prediction[0], metadata['w2idx'].keys(),self.w2v_model)\n",
    "    \n",
    "class TkinterGUIExample(tk.Tk):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create & set window variables.\n",
    "        \"\"\"\n",
    "        tk.Tk.__init__(self, *args, **kwargs)\n",
    "\n",
    "        self.title(\"Di-Feelbot\")\n",
    "        self.neuralbot = neural_chatbot()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Set window layout.\n",
    "        \"\"\"\n",
    "        self.grid()\n",
    "\n",
    "        self.respond = ttk.Button(self, text='Get Response', command=self.get_response)\n",
    "        self.respond.grid(column=0, row=0, sticky='nesw', padx=3, pady=3)\n",
    "\n",
    "        self.usr_input = ttk.Entry(self, state='normal')\n",
    "        self.usr_input.grid(column=1, row=0, sticky='nesw', padx=3, pady=3)\n",
    "\n",
    "        self.conversation_lbl = ttk.Label(self, anchor=tk.E, text='Conversation:')\n",
    "        self.conversation_lbl.grid(column=0, row=5, sticky='nesw', padx=3, pady=3)\n",
    "\n",
    "        self.conversation = ScrolledText.ScrolledText(self, state='disabled')\n",
    "        self.conversation.grid(column=0, row=6, columnspan=2, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emo_lbl = ttk.Label(self, text='Sys emotion')\n",
    "        self.emo_lbl.grid(column=0, row=1, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emoPosvalue_lbl = ttk.Label(self, text='Pos emotion',foreground='blue')\n",
    "        self.emoPosvalue_lbl.grid(column=0, row=2, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emoNegvalue_lbl = ttk.Label(self, text='Neg emotion',foreground='red')\n",
    "        self.emoNegvalue_lbl.grid(column=0, row=3, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emoNeuvalue_lbl = ttk.Label(self, text='Neu emotion',foreground='black')\n",
    "        self.emoNeuvalue_lbl.grid(column=0, row=4, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemo_lbl = ttk.Label(self, text='User emotion')\n",
    "        self.uemo_lbl.grid(column=1, row=1, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemoPosvalue_lbl = ttk.Label(self, text='Pos emotion',foreground='blue')\n",
    "        self.uemoPosvalue_lbl.grid(column=1, row=2, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemoNegvalue_lbl = ttk.Label(self, text='Neg emotion',foreground='red')\n",
    "        self.uemoNegvalue_lbl.grid(column=1, row=3, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemoNeuvalue_lbl = ttk.Label(self, text='Neu emotion',foreground='black')\n",
    "        self.uemoNeuvalue_lbl.grid(column=1, row=4, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.reinforce_lbl = ttk.Label(self, text='Reinforcement',foreground='red')\n",
    "        self.reinforce_lbl.grid(column=2, row=1, sticky='nesw', padx=3, pady=3)\n",
    "    \n",
    "        \n",
    "    def get_response(self):\n",
    "        \"\"\"\n",
    "        Get a response from the chatbot and display it.\n",
    "        \"\"\"\n",
    "        user_input = self.usr_input.get()\n",
    "        self.usr_input.delete(0, tk.END)\n",
    "\n",
    "        response = self.neuralbot.model_response(user_input)\n",
    "        x=self.neuralbot.RateSentiment(user_input)\n",
    "        self.emoPosvalue_lbl['text'] = response[0]\n",
    "        self.emoNegvalue_lbl['text'] = response[4]\n",
    "        self.emoNeuvalue_lbl['text'] = response[2]\n",
    "        self.uemoPosvalue_lbl['text'] = x[0]\n",
    "        self.uemoNegvalue_lbl['text'] = x[3]\n",
    "        self.uemoNeuvalue_lbl['text'] = str(11-int(x[0])-int(x[3]))\n",
    "        self.reinforce_lbl['text'] = self.neuralbot.reinforce + ' Reinforcement'\n",
    "        self.conversation['state'] = 'normal'\n",
    "        self.conversation.insert(\n",
    "            tk.END, \"Human: \" + user_input + \"\\n\" + \"ChatBot: \" + str(response[7:]) + \"\\n\"\n",
    "        )\n",
    "        self.conversation['state'] = 'disabled'\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "\n",
    "gui_example = TkinterGUIExample()\n",
    "gui_example.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -4\n"
     ]
    }
   ],
   "source": [
    "## Credit to http://sentistrength.wlv.ac.uk/documentation/Python.txt for the code\n",
    " \n",
    "import shlex, subprocess\n",
    " \n",
    "def RateSentiment(sentiString):\n",
    "    #open a subprocess using shlex to get the command line string into the correct args list format\n",
    "    p = subprocess.Popen(shlex.split(\"C:/ProgramData/Oracle/java/javapath/java -jar C:/Users/anul/Downloads/SentiStrength.jar stdin sentidata \\\n",
    "                                     C:/Users/anul/Downloads/Sentstrength_Data_Sept2011/ \"),\n",
    "                         stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE,universal_newlines=True)\n",
    "    #communicate via stdin the string to be rated. Note that all spaces are replaced with +\n",
    "    stdout_text, stderr_text = p.communicate(sentiString.replace(\" \",\"+\"))\n",
    "    #remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1-5\n",
    "    stdout_text = stdout_text.rstrip().replace(\"\\t\",\"\")\n",
    "     \n",
    "\n",
    "    return stdout_text\n",
    "print(RateSentiment(\"I hate you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No closing quotation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-68d7f9be5905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRateSentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-79d7cbbf50b9>\u001b[0m in \u001b[0;36mRateSentiment\u001b[0;34m(sentiString)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mRateSentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiString\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;31m#open a subprocess using shlex to get the command line string into the correct args list format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"java -jar \\'C:/Users/anul/Downloads/SentiStrength.jar stdin sentidata C:/Users/anul/Downloads/SentStrength_Data_Sept2011/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[1;31m#communicate via stdin the string to be rated. Note that all spaces are replaced with +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstdout_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiString\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\shlex.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(s, comments, posix)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommenters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\shlex.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meof\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\shlex.py\u001b[0m in \u001b[0;36mget_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[1;31m# No pushback.  Get a token.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[1;31m# Handle inclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\shlex.py\u001b[0m in \u001b[0;36mread_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shlex: I see EOF in quotes state\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[1;31m# XXX what error should be raised here?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No closing quotation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnextchar\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No closing quotation"
     ]
    }
   ],
   "source": [
    "p=RateSentiment('hate')\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 6] The handle is invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-624c32d76ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'C:/Users/anul/Downloads/SentiStrength.jar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sentidata'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C:/Users/anul/Downloads/SentStrength_Data_Sept2011/'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Any number of args to be passed to the jar file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjarWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-624c32d76ab7>\u001b[0m in \u001b[0;36mjarWrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjarWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'java'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-jar'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    840\u001b[0m                  pass_fds=()):\n\u001b[1;32m    841\u001b[0m         \u001b[1;34m\"\"\"Create new Popen instance.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[1;31m# Held while anything is calling waitpid before returncode has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[1;31m# updated to prevent clobbering returncode if wait() or poll() are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_cleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minst\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_active\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_deadstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_internal_poll\u001b[0;34m(self, _deadstate, _WaitForSingleObject, _WAIT_OBJECT_0, _GetExitCodeProcess)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \"\"\"\n\u001b[1;32m   1258\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m                 \u001b[1;32mif\u001b[0m \u001b[0m_WaitForSingleObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_WAIT_OBJECT_0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_GetExitCodeProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [WinError 6] The handle is invalid"
     ]
    }
   ],
   "source": [
    "from subprocess import *\n",
    "\n",
    "def jarWrapper(*args):\n",
    "    process = Popen(['java', '-jar']+list(args), stdout=PIPE, stderr=PIPE)\n",
    "    ret = []\n",
    "    while process.poll() is None:\n",
    "        line = process.stdout.readline()\n",
    "        if line != '' and line.endswith('\\n'):\n",
    "            ret.append(line[:-1])\n",
    "    stdout, stderr = process.communicate()\n",
    "    ret += stdout.split('\\n')\n",
    "    if stderr != '':\n",
    "        ret += stderr.split('\\n')\n",
    "    ret.remove('')\n",
    "    return ret\n",
    "\n",
    "args = ['C:/Users/anul/Downloads/SentiStrength.jar', 'text', 'hate','sentidata', 'C:/Users/anul/Downloads/SentStrength_Data_Sept2011/'] # Any number of args to be passed to the jar file\n",
    "\n",
    "result = jarWrapper(*args)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\ipykernel\\__main__.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, input_shape=(30, 300))`\n",
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\ipykernel\\__main__.py:29: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, input_shape=(30, 300))`\n",
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\ipykernel\\__main__.py:38: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\ipykernel\\__main__.py:39: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\ipykernel\\__main__.py:40: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed, Merge\n",
    "#from keras.utils.visualize_util import plot, to_graph\n",
    "#import theano.d3viz as d3v\n",
    "import word2vec_utils_1 as w2v\n",
    "import data\n",
    "import numpy as np\n",
    "from data_utils import split_dataset \n",
    "\n",
    "embed_dim = 300\n",
    "data_dim = 1\n",
    "timesteps = 26\n",
    "vocab_size = 8100\n",
    "emot_size = 4\n",
    "max_sent_length = timesteps + emot_size\n",
    "\n",
    "model_A = Sequential()\n",
    "model_A.add(LSTM(embed_dim,input_shape=(max_sent_length,embed_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "\n",
    "model_A.compile(loss='cosine_proximity',\n",
    "      optimizer='rmsprop',\n",
    "      metrics=['accuracy']\n",
    "      )\n",
    "model_A.load_weights('model_A_weights.h5')\n",
    "\n",
    "model_B = Sequential()\n",
    "model_B.add(LSTM(embed_dim,input_shape=(max_sent_length,embed_dim),return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "\n",
    "model_B.compile(loss='cosine_proximity',\n",
    "      optimizer='rmsprop',\n",
    "      metrics=['accuracy']\n",
    "      )\n",
    "model_B.load_weights('model_B_weights.h5')\n",
    "\n",
    "model_ABA = Sequential()\n",
    "model_ABA.add(Merge([model_A, model_B], mode = 'concat'))\n",
    "model_ABA.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "model_ABA.add(LSTM(embed_dim,return_sequences=True, dropout_W=0.2, dropout_U=0.2))\n",
    "#model_ABA.add(TimeDistributed(Dense(vocab_size,activation='softmax')))\n",
    "\n",
    "model_ABA.compile(loss='cosine_proximity',\n",
    "      optimizer='rmsprop',\n",
    "      metrics=['accuracy']\n",
    "      )\n",
    "model_ABA.load_weights('model_ABA_weights.h5')\n",
    "\n",
    "\n",
    "def cosign_similarity( a, b):\n",
    "    dot = np.dot(a,b)\n",
    "    norm = np.linalg.norm(a-b)\n",
    "    return( dot/norm )\n",
    "\n",
    "def most_likely_word( prediction, words, w2v_model ):\n",
    "    ret = \"\"\n",
    "    max_so_far = cosign_similarity( np.array([1]+[0]*299),prediction ) #must be better than blank\n",
    "    #Best word in dictionary\n",
    "    for word in words:\n",
    "        similarity = cosign_similarity( w2v.vectorize(w2v_model,word)[0], prediction )\n",
    "        if similarity > max_so_far:\n",
    "            max_so_far = similarity\n",
    "            ret = word    \n",
    "    return( ret )\n",
    "\n",
    "def most_likely_sent( prediction_vec, words, w2v_model ):\n",
    "    predicted_words = []\n",
    "    for prediction in prediction_vec:\n",
    "        predicted_words.append( most_likely_word(prediction,words,w2v_model))\n",
    "    ret = \" \".join(predicted_words)\n",
    "    return( ret )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 30, 300)           721200    \n",
      "=================================================================\n",
      "Total params: 721,200\n",
      "Trainable params: 721,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = w2v.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v.save_unknown_vectors({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_unknown_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v.unvectorize_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "the STRING opcode argument must be quoted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fefd1d3d2c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#unknown_vectors=[]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_A1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_A2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mUser_A1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"5 5 1 @I really love you. I think I want to marry you!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mUser_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"1 5 5 @yeah well I hate you and never want to see you again! \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mUser_A1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUser_A1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw2v_model\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpad_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_sent_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\anul\\aibot\\EmotionalChatBot\\data.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(PATH)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[1;31m# read data control dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'metadata.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[1;31m# read numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'A1.npy'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: the STRING opcode argument must be quoted"
     ]
    }
   ],
   "source": [
    "#unknown_vectors=[]\n",
    "metadata, filtered_A1, filtered_B, filtered_A2 = data.load_data()\n",
    "User_A1 = \"5 5 1 @I really love you. I think I want to marry you!\"\n",
    "User_B = \"1 5 5 @yeah well I hate you and never want to see you again! \"\n",
    "User_A1 = w2v.vectorize( sentence=User_A1,model=w2v_model , pad_length = max_sent_length) \n",
    "User_B  = w2v.vectorize( sentence=User_B,model=w2v_model,  pad_length = max_sent_length)\n",
    "print(np.array([User_B[0]]).shape)\n",
    "user_input = [ np.array( [User_A1]), np.array([User_B]) ]\n",
    "prediction = model_ABA.predict(user_input)\n",
    "print(most_likely_sent( prediction[0], metadata['w2idx'].keys(),w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "model_json = model_ABA.to_json()\n",
    "with open(\"model_ABA.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_ABA.save_weights(\"model_ABA.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\anul\\Anaconda2\\envs\\python2\\lib\\lib-tk\\Tkinter.py\", line 1542, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-2-de1a6c221b0b>\", line 94, in get_response\n",
      "    time.sleep(0.5)\n",
      "NameError: global name 'time' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\anul\\Anaconda2\\envs\\python2\\lib\\lib-tk\\Tkinter.py\", line 1542, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-2-de1a6c221b0b>\", line 94, in get_response\n",
      "    time.sleep(0.5)\n",
      "NameError: global name 'time' is not defined\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import Tkinter as tk\n",
    "try:\n",
    "    import ttk as ttk\n",
    "    import ScrolledText\n",
    "except ImportError:\n",
    "    import tkinter.ttk as ttk\n",
    "    import tkinter.scrolledtext as ScrolledText\n",
    "class TkinterGUIExample(tk.Tk):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Create & set window variables.\n",
    "        \"\"\"\n",
    "        tk.Tk.__init__(self, *args, **kwargs)\n",
    "\n",
    "        self.title(\"Di-Feelbot\")\n",
    "        #self.neuralbot = neural_chatbot()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Set window layout.\n",
    "        \"\"\"\n",
    "        self.grid()\n",
    "\n",
    "        self.respond = ttk.Button(self, text='Get Response', command=self.get_response)\n",
    "        self.respond.grid(column=0, row=0, sticky='nesw', padx=3, pady=3)\n",
    "\n",
    "        self.usr_input = ttk.Entry(self, state='normal')\n",
    "        self.usr_input.grid(column=1, row=0, sticky='nesw', padx=3, pady=3)\n",
    "\n",
    "        self.conversation_lbl = ttk.Label(self, anchor=tk.E, text='Conversation:')\n",
    "        self.conversation_lbl.grid(column=0, row=5, sticky='nesw', padx=3, pady=3)\n",
    "\n",
    "        self.conversation = ScrolledText.ScrolledText(self, state='disabled')\n",
    "        self.conversation.grid(column=0, row=6, columnspan=2, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emo_lbl = ttk.Label(self, text='Sys emotion')\n",
    "        self.emo_lbl.grid(column=0, row=1, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emoPosvalue_lbl = ttk.Label(self, text='Pos emotion',foreground='blue')\n",
    "        self.emoPosvalue_lbl.grid(column=0, row=2, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emoNegvalue_lbl = ttk.Label(self, text='Neg emotion',foreground='red')\n",
    "        self.emoNegvalue_lbl.grid(column=0, row=3, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.emoNeuvalue_lbl = ttk.Label(self, text='Neu emotion',foreground='black')\n",
    "        self.emoNeuvalue_lbl.grid(column=0, row=4, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemo_lbl = ttk.Label(self, text='User emotion')\n",
    "        self.uemo_lbl.grid(column=1, row=1, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemoPosvalue_lbl = ttk.Label(self, text='Pos emotion',foreground='blue')\n",
    "        self.uemoPosvalue_lbl.grid(column=1, row=2, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemoNegvalue_lbl = ttk.Label(self, text='Neg emotion',foreground='red')\n",
    "        self.uemoNegvalue_lbl.grid(column=1, row=3, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "        self.uemoNeuvalue_lbl = ttk.Label(self, text='Neu emotion',foreground='black')\n",
    "        self.uemoNeuvalue_lbl.grid(column=1, row=4, sticky='nesw', padx=3, pady=3)\n",
    "        \n",
    "    def RateSentiment(self,sentiString):\n",
    "        #open a subprocess using shlex to get the command line string into the correct args list format\n",
    "        p = subprocess.Popen(shlex.split(\"C:/ProgramData/Oracle/java/javapath/java -jar C:/Users/anul/Downloads/SentiStrength.jar stdin sentidata C:/Users/anul/Downloads/SentStrength_Data_Sept2011/\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "        #communicate via stdin the string to be rated. Note that all spaces are replaced with +\n",
    "        stdout_text, stderr_text = p.communicate(sentiString.replace(\" \",\"+\"))\n",
    "        #remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1-5\n",
    "        stdout_text = stdout_text.rstrip().replace(\"\\t\",\"\")\n",
    "        return stdout_text\n",
    "        \n",
    "    def get_response(self):\n",
    "        \"\"\"\n",
    "        Get a response from the chatbot and display it.\n",
    "        \"\"\"\n",
    "        user_input = self.usr_input.get()\n",
    "        self.usr_input.delete(0, tk.END)\n",
    "\n",
    "        response = '1 9 1 @Hi how are you'#self.neuralbot.model_response(user_input)\n",
    "        x=self.RateSentiment(user_input)\n",
    "        self.emoPosvalue_lbl['text'] = response[0]\n",
    "        self.emoNegvalue_lbl['text'] = response[4]\n",
    "        self.emoNeuvalue_lbl['text'] = response[2]\n",
    "        self.uemoPosvalue_lbl['text'] = x[0]\n",
    "        self.uemoNegvalue_lbl['text'] = x[3]\n",
    "        self.uemoNeuvalue_lbl['text'] = str(11-int(x[0])-int(x[3]))\n",
    "        self.conversation['state'] = 'normal'\n",
    "        self.conversation.insert(\n",
    "            tk.END, \"Human: \" + user_input + \"\\n\" + \"ChatBot: \" + str(response[7:]) + \"\\n\"\n",
    "        )\n",
    "        self.conversation['state'] = 'disabled'\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "\n",
    "gui_example = TkinterGUIExample()\n",
    "gui_example.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\anul\\Anaconda2\\envs\\python35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30, 101)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30, 101)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 30, 202)       164024      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 30, 202)       164024      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "A1_layer2 (LSTM)                 (None, 30, 202)       327240      bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "B_layer2 (LSTM)                  (None, 30, 202)       327240      bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 404)       0           A1_layer2[0][0]                  \n",
      "                                                                   B_layer2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "A2_layer1 (LSTM)                 (None, 30, 404)       1307344     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 10003)     4051215     A2_layer1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,341,087\n",
      "Trainable params: 6,341,087\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed, concatenate, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import word2vec_utils as w2v\n",
    "import data\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from data_utils import split_dataset \n",
    "from chat_constants import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "LSTM_DROPOUT = 0.15\n",
    "A1_input = Input(shape=(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "A1_layer1 = Bidirectional( LSTM(EMBED_DIM,name = \"A1_layer1\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT) )\n",
    "A1_layer2 = LSTM(EMBED_DIM*2,name = \"A1_layer2\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "A1_net = A1_layer2(A1_layer1(A1_input))\n",
    "\n",
    "B_input = Input(shape=(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "B_layer1 = Bidirectional( LSTM(EMBED_DIM,name = \"B_layer1\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT) )\n",
    "B_layer2 = LSTM(EMBED_DIM*2,name = \"B_layer2\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "B_net = B_layer2(B_layer1(B_input))\n",
    "\n",
    "combined = concatenate([A1_net,B_net])\n",
    "A2_pred1 = LSTM(EMBED_DIM*4, name = \"A2_layer1\", return_sequences = True, dropout = LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "A2_pred2 = TimeDistributed(Dense(VOCAB_SIZE + 3, name = \"A2_layer2\", activation = 'softmax' ) )\n",
    "\n",
    "A2_net =A2_pred2(A2_pred1(combined))\n",
    "\n",
    "one_hot_chat_model = Model(inputs = [ A1_input,B_input], outputs = [A2_net])\n",
    "adam = keras.optimizers.Adam(lr = 0.01)#default 0.001\n",
    "one_hot_chat_model.compile( optimizer=adam,loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "one_hot_chat_model.summary()\n",
    "one_hot_chat_model.load_weights(\"one_hot_chat_net.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = one_hot_chat_model.to_json()\n",
    "with open(\"one_hot_chat_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "one_hot_chat_model.save_weights(\"one_hot_chat_net.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b8146daa09b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_freqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'words_in_order_of_freq.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_freqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "word_freqs = np.load('words_in_order_of_freq.npy') \n",
    "print(word_freqs.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
