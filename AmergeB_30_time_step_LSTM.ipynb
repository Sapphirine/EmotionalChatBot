{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed, Merge, Input, concatenate\n",
    "#from keras.utils.visualize_util import plot, to_graph\n",
    "#import theano.d3viz as d3v\n",
    "import word2vec_utils as w2v\n",
    "import data\n",
    "import numpy as np\n",
    "from data_utils import split_dataset \n",
    "from chat_constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = w2v.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert( len(w2v_model.vocab) == 3000000 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_DROPOUT = 0.15\n",
    "A1, B, A2 = data.load_data()\n",
    "def get_sets_of_data(size=5000):\n",
    "    A1, B, A2 = data.load_data()\n",
    "    All_data = np.array([A1,B,A2])\n",
    "    num_sections = len(A1)//size\n",
    "    sets = []\n",
    "    for i in range(num_sections):\n",
    "        sets.append(All_data[:,i*size:(i+1)*size])\n",
    "    return(sets)\n",
    "sets = get_sets_of_data()\n",
    "adam = keras.optimizers.Adam(lr = 0.025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A Layer Pre Train ### \n",
    "A_Input = Input(shape =(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "A_layer1 = LSTM(EMBED_DIM,return_sequences=True, name = \"layer_a\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "a_pretrain = Model( inputs = [A_Input], outputs = [ A_layer1(A_Input)])\n",
    "a_pretrain.load_weights('AmergeB_len30_301dim_w2v_in_and_out.h5',by_name = True)\n",
    "a_pretrain.compile( optimizer='rmsprop', loss = 'cosine_proximity',metrics = ['accuracy'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 24s - loss: -0.0021 - acc: 0.4847    \n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 23s - loss: -0.0021 - acc: 0.4852    \n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 24s - loss: -0.0021 - acc: 0.4849    \n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 25s - loss: -0.0021 - acc: 0.4854    \n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 22s - loss: -0.0021 - acc: 0.4855    \n"
     ]
    }
   ],
   "source": [
    "a1,b1,a2 = sets[3]\n",
    "A1_train,B_train,A2_train = w2v.get_training_data(a1,b,a2)\n",
    "a_pretrain.load_weights('a_pretrain.h5',by_name=True)\n",
    "a_pretrain.fit([A1_train],A2_train, batch_size = 200, epochs = 5)\n",
    "a_pretrain.save('a_pretrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A Layer Pre Train ### \n",
    "B_Input = Input(shape =(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "B_layer1 = LSTM(EMBED_DIM,return_sequences=True, name = \"layer_b\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "b_pretrain = Model( inputs = [B_Input], outputs = [ B_layer1(B_Input)])\n",
    "#b_pretrain.load_weights('AmergeB_len30_301dim_w2v_in_and_out.h5',by_name = True)\n",
    "b_pretrain.compile( optimizer='rmsprop', loss = 'cosine_proximity',metrics = ['accuracy'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 25s - loss: -0.0014 - acc: 0.4564    \n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 22s - loss: -0.0020 - acc: 0.5000    \n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 22s - loss: -0.0021 - acc: 0.5122    \n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 26s - loss: -0.0021 - acc: 0.5210    \n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 27s - loss: -0.0022 - acc: 0.5278    \n"
     ]
    }
   ],
   "source": [
    "a1,b1,a2 = sets[4]\n",
    "A1_train,B_train,A2_train = w2v.get_training_data(a1,b,a2)\n",
    "b_pretrain.fit([B_train],A2_train, batch_size = 200, epochs = 5)\n",
    "b_pretrain.save('b_pretrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 30, 301)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_16 (InputLayer)            (None, 30, 301)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_17 (InputLayer)            (None, 30, 301)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "layer_a (LSTM)                   (None, 30, 301)       726012                                       \n",
      "____________________________________________________________________________________________________\n",
      "layer_b (LSTM)                   (None, 30, 301)       726012                                       \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 30, 602)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "layer_ab1 (LSTM)                 (None, 30, 301)       1088416                                      \n",
      "____________________________________________________________________________________________________\n",
      "layer_ab2 (LSTM)                 (None, 30, 301)       726012                                       \n",
      "====================================================================================================\n",
      "Total params: 3,266,452\n",
      "Trainable params: 1,814,428\n",
      "Non-trainable params: 1,452,024\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "A_Input = Input(shape =(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "B_Input = Input(shape =(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "A_layer1 = LSTM(EMBED_DIM,return_sequences=True, name = \"layer_a\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "B_layer1 = LSTM(EMBED_DIM,return_sequences=True, name = \"layer_b\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "A_layer1.trainable = False\n",
    "B_layer1.trainable = False\n",
    "merge    = concatenate( [A_layer1(A_Input),B_layer1(B_Input)])\n",
    "\n",
    "AB_layer1 = LSTM(EMBED_DIM,return_sequences=True, name = \"layer_ab1\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "AB_layer2 = LSTM(EMBED_DIM,return_sequences=True, name = \"layer_ab2\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "AB_output = AB_layer2(AB_layer1(merge))\n",
    "\n",
    "chat_model = Model(inputs = [ A_Input,B_Input], outputs = [AB_output])\n",
    "\n",
    "chat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chat_model.load_weights('a_pretrain.h5',by_name=True)\n",
    "chat_model.load_weights('b_pretrain.h5',by_name=True)\n",
    "chat_model.load_weights('AmergeB_len30_30dim_w2v_in_and_out.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.compile( optimizer='rmsprop',loss = 'cosine_proximity',metrics = ['accuracy'])\n",
    "for a1,b,a2 in sets:\n",
    "    A1_train,B_train,A2_train = w2v.get_training_data(a1,b,a2)\n",
    "    chat_model.fit([A1_train,B_train], A2_train,\n",
    "          batch_size=200, epochs=5,\n",
    "          validation_split = 0.05 )\n",
    "    chat_model.save('AmergeB_len30_301dim_w2v_in_and_out.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 1 @ Well , I thought we'd start with pronunciation , if that's okay with you .\n",
      "2 8 1 @ Not the hacking and gagging and spitting part . Please .\n",
      "1 9 1 @ Okay . . . then how 'bout we try out some French cuisine . Saturday ? Night ?\n",
      "('w2v word-score', u'1', 0.9904819130897522, '\\n', 'uk word-score', 'at-arms', 0.25018321554559492)\n",
      "('w2v word-score', u'9', 0.9829464554786682, '\\n', 'uk word-score', 'at-arms', 0.24525209555474739)\n",
      "('w2v word-score', u'1', 0.9902269840240479, '\\n', 'uk word-score', 'at-arms', 0.24893820822454898)\n",
      "('w2v word-score', u'@', 0.9984010457992554, '\\n', 'uk word-score', 'aboard--you', 0.20060584535994638)\n",
      "('w2v word-score', u'You', 0.7782272100448608, '\\n', 'uk word-score', 'thirty--', 0.20711186238067558)\n",
      "('w2v word-score', u'do', 0.7360661029815674, '\\n', 'uk word-score', \"carl's\", 0.21274558816409631)\n",
      "('w2v word-score', u'do', 0.7273008227348328, '\\n', 'uk word-score', '.', 0.23543978289334419)\n",
      "('w2v word-score', u'do', 0.7091096639633179, '\\n', 'uk word-score', '.', 0.28369220309557974)\n",
      "('w2v word-score', u'just', 0.698669970035553, '\\n', 'uk word-score', '.', 0.32617009183718337)\n",
      "('w2v word-score', u'just', 0.6870971918106079, '\\n', 'uk word-score', '.', 0.36680573731512311)\n",
      "('w2v word-score', u'just', 0.673917293548584, '\\n', 'uk word-score', '.', 0.40473492599868149)\n",
      "('w2v word-score', u'just', 0.6622377038002014, '\\n', 'uk word-score', '.', 0.4347829790620209)\n",
      "('w2v word-score', u'just', 0.6526203155517578, '\\n', 'uk word-score', '.', 0.45829023399007002)\n",
      "('w2v word-score', u'just', 0.6466772556304932, '\\n', 'uk word-score', '.', 0.47086343387581581)\n",
      "('w2v word-score', u'anyway', 0.6426776051521301, '\\n', 'uk word-score', '.', 0.479853082881747)\n",
      "('w2v word-score', u'anyway', 0.6417574882507324, '\\n', 'uk word-score', '.', 0.48485229833149029)\n",
      "('w2v word-score', u'anyway', 0.6406227350234985, '\\n', 'uk word-score', '.', 0.488309804728287)\n",
      "('w2v word-score', u'anyway', 0.6389076709747314, '\\n', 'uk word-score', '.', 0.49165242716900703)\n",
      "('w2v word-score', u'anyway', 0.636532187461853, '\\n', 'uk word-score', '.', 0.49504989567839053)\n",
      "1 9 1 @ You do do do just just just just just just anyway anyway anyway anyway anyway _ _ _ _ _ _ _ _ _ _ _\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "A1_test,B_test,A2_test = w2v.get_training_data(A1[i:i+1],B[i:i+1],A2[i:i+1])\n",
    "print(A1[i])\n",
    "print(B[i])\n",
    "print(A2[i])\n",
    "\n",
    "predicted = chat_model.predict([A1_test,B_test])\n",
    "predicted_words = w2v.unvectorize_sentence( predicted[0] )\n",
    "print( predicted_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
