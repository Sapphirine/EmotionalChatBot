{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed, Concatenate, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import word2vec_utils as w2v\n",
    "import data\n",
    "import numpy as np\n",
    "import re\n",
    "from data_utils import split_dataset \n",
    "from chat_constants import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### TO RUN Set blank to be 300*[0] + [1]\n",
    "##### Set w2v to google embedings before running the whole script\n",
    "assert( EMBED_DIM == 301 )\n",
    "w2v_model = w2v.initialize()\n",
    "assert( len( w2v_model.vocab) == 3000000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def null_punisher(y_true, y_pred ):\n",
    "    '''Provides stronger incentive to avoid Null'''\n",
    "    L = 3\n",
    "    error = keras.losses.cosine_proximity( y_true,y_pred)\n",
    "    if y_true != BLANK:\n",
    "        error *= L\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60, 301)           0         \n",
      "_________________________________________________________________\n",
      "AB_layer1 (LSTM)             (None, 60, 301)           726012    \n",
      "_________________________________________________________________\n",
      "AB_layer2 (LSTM)             (None, 60, 301)           726012    \n",
      "_________________________________________________________________\n",
      "AB_layer3 (LSTM)             (None, 60, 301)           726012    \n",
      "=================================================================\n",
      "Total params: 2,178,036\n",
      "Trainable params: 2,178,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_DROPOUT = 0.1\n",
    "AB_Input = Input(shape =(MAX_SENT_LENGTH*2,EMBED_DIM))\n",
    "\n",
    "AB_layer1 = LSTM(EMBED_DIM,return_sequences=True, name = \"AB_layer1\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "AB_layer2 = LSTM(EMBED_DIM,return_sequences=True, name = \"AB_layer2\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "AB_layer3 = LSTM(EMBED_DIM,return_sequences=True, name = \"AB_layer3\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "AB_output = AB_layer3(AB_layer2(AB_layer1(AB_Input)))\n",
    "\n",
    "chat_model = Model(inputs = [ AB_Input], outputs = [AB_output])\n",
    "chat_model.compile(loss=null_punisher,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "chat_model.load_weights('AB_len60_301dim_w2v_in_and_out.h5',by_name=True)\n",
    "chat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.unvectorize_initialize()\n",
    "A1, B, A2 = data.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "A1_test,B_test,A2_test = w2v.get_training_data(A1[i:i+1],B[i:i+1],A2[i:i+1])\n",
    "A1B_test = np.concatenate((A1_test,B_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 1 @ Well , I thought we'd start with pronunciation , if that's okay with you .\n",
      "2 8 1 @ Not the hacking and gagging and spitting part . Please .\n",
      "1 9 1 @ Okay . . . then how 'bout we try out some French cuisine . Saturday ? Night ?\n"
     ]
    }
   ],
   "source": [
    "print(A1[i])\n",
    "print(B[i])\n",
    "print(A2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w2v word-score', u'1', 0.9980672597885132, '\\n', 'uk word-score', 'at-arms', 0.24375356828796638)\n",
      "('w2v word-score', u'@', 0.9984095692634583, '\\n', 'uk word-score', 'aboard--you', 0.20282582263920168)\n",
      "('w2v word-score', u'Not', 0.9961111545562744, '\\n', 'uk word-score', \"it's-\", 0.1997054688543958)\n",
      "('w2v word-score', u'the', 0.995297908782959, '\\n', 'uk word-score', 'pre-empt', 0.21089029170384549)\n",
      "('w2v word-score', u'hacking', 0.9780434370040894, '\\n', 'uk word-score', 'wo-man', 0.22321916042219775)\n",
      "('w2v word-score', u'Darmer', 0.3125925064086914, '\\n', 'uk word-score', 'gutrush', 0.19659303532697145)\n",
      "('w2v word-score', u'gagging', 0.8653687238693237, '\\n', 'uk word-score', \"neil's\", 0.18443881196043233)\n",
      "('w2v word-score', u'Regal_Beagle', 0.3066844642162323, '\\n', 'uk word-score', 'gutrush', 0.18993413025796788)\n",
      "('w2v word-score', u'spitting', 0.9315758943557739, '\\n', 'uk word-score', 'try-', 0.19629132881455147)\n",
      "('w2v word-score', u'part', 0.9129929542541504, '\\n', 'uk word-score', \"'nice\", 0.24466628336354451)\n",
      "('w2v word-score', u'part', 0.31119364500045776, '\\n', 'uk word-score', 'denfert', 0.17845849424707966)\n",
      "('w2v word-score', u'Please', 0.794174075126648, '\\n', 'uk word-score', \"'to\", 0.23293298431507448)\n",
      "('w2v word-score', u'asap', 0.2547250986099243, '\\n', 'uk word-score', '!', 0.23450025332331029)\n",
      "('w2v word-score', u'Please', 0.44129788875579834, '\\n', 'uk word-score', \"what'll\", 0.23319915596981522)\n",
      "('w2v word-score', u'Stones', 0.2722066044807434, '\\n', 'uk word-score', \"tom's\", 0.1968989389557225)\n",
      "('w2v word-score', u'DePuy_ASR_Hip_Resurfacing', 0.2705974578857422, '\\n', 'uk word-score', \"part's\", 0.2005723147459029)\n",
      "('w2v word-score', u'DePuy_ASR_Hip_Resurfacing', 0.27283135056495667, '\\n', 'uk word-score', 'sea-', 0.20906313438256519)\n",
      "('w2v word-score', u'Cutarelli', 0.27203503251075745, '\\n', 'uk word-score', \"movie's\", 0.19241800337217002)\n",
      "('w2v word-score', u'Escovedo', 0.2591395378112793, '\\n', 'uk word-score', \"movie's\", 0.23287756396611276)\n",
      "('w2v word-score', u'Escovedo', 0.27670833468437195, '\\n', 'uk word-score', \"movie's\", 0.22587837086202753)\n",
      "('w2v word-score', u'Escovedo', 0.2792053818702698, '\\n', 'uk word-score', \"movie's\", 0.21852766252157893)\n",
      "('w2v word-score', u'Escovedo', 0.27979758381843567, '\\n', 'uk word-score', \"movie's\", 0.20870876829210691)\n",
      "('w2v word-score', u'Minolta_camera', 0.2691168785095215, '\\n', 'uk word-score', 'level--', 0.21179600282448138)\n",
      "2 8 1 @ Not the hacking Darmer gagging Regal_Beagle spitting part part Please asap Please Stones DePuy_ASR_Hip_Resurfacing DePuy_ASR_Hip_Resurfacing Cutarelli Escovedo Escovedo Escovedo Escovedo Minolta_camera _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted = chat_model.predict(A1B_test)\n",
    "predicted_words = w2v.unvectorize_sentence( predicted[0] )\n",
    "print( predicted_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
