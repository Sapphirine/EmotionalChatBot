{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed, concatenate, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import word2vec_utils as w2v\n",
    "import data\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from data_utils import split_dataset \n",
    "from chat_constants import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = w2v.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def null_punisher(y_true, y_pred ):\n",
    "    '''Provides stronger incentive to avoid Null'''\n",
    "    L = 3\n",
    "    error = keras.losses.cosine_proximity( y_true,y_pred)\n",
    "    if y_true[-1] != BLANK[-1] or y_pred[-1] == BLANK[-1]:\n",
    "        error *= L\n",
    "    return error\n",
    "\n",
    "def get_sets_of_data(size=5000):\n",
    "    A1, B, A2 = data.load_data()\n",
    "    All_data = np.array([A1,B,A2])\n",
    "    num_sections = len(A1)//size\n",
    "    sets = []\n",
    "    for i in range(num_sections):\n",
    "        sets.append(All_data[:,i*size:(i+1)*size])\n",
    "    return(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_DROPOUT = 0.15\n",
    "A1, B, A2 = data.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 30, 101)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 30, 101)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional) (None, 30, 202)       164024                                       \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional) (None, 30, 202)       164024                                       \n",
      "____________________________________________________________________________________________________\n",
      "A1_layer2 (LSTM)                 (None, 30, 101)       122816                                       \n",
      "____________________________________________________________________________________________________\n",
      "B_layer2 (LSTM)                  (None, 30, 101)       122816                                       \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 30, 202)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "A2_layer1 (LSTM)                 (None, 30, 101)       122816                                       \n",
      "====================================================================================================\n",
      "Total params: 696,496\n",
      "Trainable params: 696,496\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "A1_input = Input(shape=(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "A1_layer1 = Bidirectional( LSTM(EMBED_DIM,name = \"A1_layer1\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT) )\n",
    "A1_layer2 = LSTM(EMBED_DIM,name = \"A1_layer2\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "A1_net = A1_layer2(A1_layer1(A1_input))\n",
    "\n",
    "B_input = Input(shape=(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "B_layer1 = Bidirectional( LSTM(EMBED_DIM,name = \"B_layer1\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT) )\n",
    "B_layer2 = LSTM(EMBED_DIM,name = \"B_layer2\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "B_net = B_layer2(B_layer1(B_input))\n",
    "\n",
    "combined = concatenate([A1_net,B_net])\n",
    "A2_pred1 = LSTM(EMBED_DIM, name = \"A2_layer1\", return_sequences = True, dropout = LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "A2_net = A2_pred1(combined)\n",
    "\n",
    "chat_model = Model( inputs = [ A1_input,B_input ], outputs = [ A2_net ] )\n",
    "chat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint( 'chat_net.h5',verbose = 1,monitor = 'val_acc',save_best_only = True)\n",
    "early_stopping = EarlyStopping( monitor = 'val_acc',patience = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets = get_sets_of_data(size = 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7600 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "7400/7600 [============================>.] - ETA: 2s - loss: -0.0169 - acc: 0.4049Epoch 00000: val_acc improved from 0.09133 to 0.51742, saving model to chat_net.h5\n",
      "7600/7600 [==============================] - 94s - loss: -0.0170 - acc: 0.4071 - val_loss: -0.0196 - val_acc: 0.5174\n",
      "Epoch 2/5\n",
      "7400/7600 [============================>.] - ETA: 2s - loss: -0.0192 - acc: 0.4924Epoch 00001: val_acc improved from 0.51742 to 0.51817, saving model to chat_net.h5\n",
      "7600/7600 [==============================] - 93s - loss: -0.0192 - acc: 0.4927 - val_loss: -0.0198 - val_acc: 0.5182\n",
      "Epoch 3/5\n",
      "7400/7600 [============================>.] - ETA: 2s - loss: -0.0194 - acc: 0.4934Epoch 00002: val_acc did not improve\n",
      "7600/7600 [==============================] - 93s - loss: -0.0193 - acc: 0.4927 - val_loss: -0.0198 - val_acc: 0.5180\n",
      "Epoch 4/5\n",
      "7400/7600 [============================>.] - ETA: 2s - loss: -0.0194 - acc: 0.5126Epoch 00003: val_acc improved from 0.51817 to 0.56775, saving model to chat_net.h5\n",
      "7600/7600 [==============================] - 92s - loss: -0.0194 - acc: 0.5136 - val_loss: -0.0199 - val_acc: 0.5678\n",
      "Epoch 5/5\n",
      "7400/7600 [============================>.] - ETA: 2s - loss: -0.0195 - acc: 0.5428Epoch 00004: val_acc improved from 0.56775 to 0.56875, saving model to chat_net.h5\n",
      "7600/7600 [==============================] - 102s - loss: -0.0195 - acc: 0.5428 - val_loss: -0.0199 - val_acc: 0.5687\n",
      "Train on 7600 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "6200/7600 [=======================>......] - ETA: 18s - loss: -0.0195 - acc: 0.5460"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-4cb98ae3fbda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           callbacks = [checkpointer,early_stopping] )\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/paperspace/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:6179)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mvalue_zeros\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "adam = keras.optimizers.Adam(lr = 0.025)\n",
    "chat_model.compile( optimizer=adam,loss = null_punisher,metrics = ['accuracy'])\n",
    "chat_model.load_weights('chat_net.h5',by_name = True)\n",
    "for a1,b,a2 in sets:\n",
    "    A1_train,B_train,A2_train = w2v.get_training_data(a1,b,a2)\n",
    "    chat_model.fit([A1_train,B_train], A2_train,\n",
    "          batch_size=200, epochs=5,\n",
    "          validation_split = 0.05,\n",
    "          callbacks = [checkpointer,early_stopping] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freqs = np.load('words_in_order_of_freq.npy')[:VOCAB_SIZE]\n",
    "embeded_words = np.array(map( lambda x: np.append(w2v_model[x],0), word_freqs ) )\n",
    "categorical_words = np.array( [ to_categorical(x,num_classes = VOCAB_SIZE + 3)[0] for x in range(len(word_freqs)) ] )\n",
    "w2v_to_onehot = Sequential()\n",
    "w2v_to_onehot.add(Dense(VOCAB_SIZE+3, input_shape=(EMBED_DIM,),activation = 'softmax',name='w2v_to_onehot_mapper'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr = 0.025)#default 0.001\n",
    "w2v_to_onehot.compile( optimizer=adam,loss = 'categorical_crossentropy',metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 10s - loss: 5.8608 - categorical_accuracy: 0.6286    \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 10s - loss: 5.8563 - categorical_accuracy: 0.6271    \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 9s - loss: 5.7873 - categorical_accuracy: 0.6398     \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 11s - loss: 5.7601 - categorical_accuracy: 0.6426    \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 10s - loss: 5.7592 - categorical_accuracy: 0.6426    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe5a5d63d0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_to_onehot.load_weights('w2v_to_onehot.h5',by_name=True)\n",
    "w2v_to_onehot.fit([embeded_words],categorical_words,batch_size = 200, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "i = 78\n",
    "word=w2v_model[word_freqs[i]]\n",
    "word = np.append(word,0)\n",
    "\n",
    "word_vec = w2v_to_onehot.predict(np.array([word]))\n",
    "print( word_freqs[i])\n",
    "print(word_freqs[np.where(word_vec==max(word_vec))[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_to_onehot.save('w2v_to_onehot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def just_model_un_vectorize( predicted ):\n",
    "    ret =[]\n",
    "    for word_vec in predicted:\n",
    "        if abs(word_vec[-1] - 1) < 0.001:\n",
    "            ret.append(\"_\")\n",
    "            continue\n",
    "        word_vec = word_vec[:-1]\n",
    "        x = w2v_model.similar_by_vector( word_vec, topn = 3 )\n",
    "        print( x )\n",
    "        w2v_word,w2v_similarity = x[0]\n",
    "        print( w2v_word, \" \", w2v_similarity )\n",
    "        ret.append(w2v_word)\n",
    "    return( \" \".join(ret) )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 8 1 @ I'll slip in , talk them into to come out , and you'll be free to blow holy high heaven the whole lot of them .\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.unvectorize_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = chat_model.predict([A1_test,B_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_words = just_model_un_vectorize( predicted[0] )\n",
    "print( predicted_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model.most_similar('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLANK.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {}\n",
    "a['__BLANK__'] = BLANK\n",
    "a['__BLANK__']\n",
    "\n",
    "import pickle\n",
    "with open('unknown_words_stored.pkl', 'w+') as f:\n",
    "    pickle.dump(a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( A1[30000], B[30000],A2[30000] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model.similar_by_word(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AB_Input = Input(shape =(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "AB_layer1 = LSTM(EMBED_DIM,return_sequences=True, name = \"AB_layer1\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "AB_layer2 = LSTM(EMBED_DIM,return_sequences=True, name = \"AB_layer2\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "AB_layer3 = LSTM(EMBED_DIM,return_sequences=True, name = \"AB_layer3\", dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "AB_output = AB_layer3(AB_layer2(AB_layer1(AB_Input)))\n",
    "\n",
    "chat_model = Model(inputs = [ AB_Input], outputs = [AB_output])\n",
    "chat_model.compile(loss=null_punisher,\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "early_stopping = EarlyStopping( monitor = 'val_acc',patience = 2)\n",
    "chat_model.load_weights('chat_net.h5',by_name=True)\n",
    "chat_model.fit([A1B_train], A2_train,\n",
    "          batch_size=200, epochs=20,\n",
    "          validation_split = 0.025 )\n",
    "chat_model.save('chat_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input = Input(shape=(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "layer1 = Bidirectional(LSTM(EMBED_DIM,name = \"layer1\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT))\n",
    "layer2 = Bidirectional(LSTM(EMBED_DIM,name = \"layer2\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT))\n",
    "layer3 = LSTM(EMBED_DIM,name = \"layer3\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "my_output = layer3(layer2(layer1(my_input)))\n",
    "\n",
    "simpl_chat_model = Model(inputs = [ my_input], outputs = [my_output])\n",
    "simpl_chat_model.summary()\n",
    "\n",
    "simpl_checkpointer = ModelCheckpoint( 'simpl_chat_net.h5',verbose = 1,monitor = 'val_acc',save_best_only = True)\n",
    "simpl_early_stopping = EarlyStopping( monitor = 'val_acc',patience = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr = 0.02)\n",
    "simpl_chat_model.compile( optimizer=adam,loss = null_punisher,metrics = ['accuracy'])\n",
    "#simpl_chat_model.load_weights('simpl_chat_net.h5',by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a1,b,a2 in sets:\n",
    "    A1_train,B_train,A2_train = w2v.get_training_data(a1,b,a2)\n",
    "    simpl_chat_model.fit([A1_train], B_train,\n",
    "          batch_size=200, epochs=1,\n",
    "          validation_split = 0.05,\n",
    "          callbacks = [simpl_checkpointer,simpl_early_stopping] )\n",
    "    simpl_chat_model.fit([B_train], A2_train,\n",
    "          batch_size=200, epochs=1,\n",
    "          validation_split = 0.05,\n",
    "          callbacks = [simpl_checkpointer,simpl_early_stopping] )\n",
    "    #chat_model.save('chat_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predit = simpl_chat_model.predict([B_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_words = just_model_un_vectorize( predit[0] )\n",
    "print( predicted_words )"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 5,
   "metadata": {},

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30, 301)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30, 301)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 30, 602)       1452024                                      \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 30, 602)       1452024                                      \n",
      "____________________________________________________________________________________________________\n",
      "A1_layer2 (LSTM)                 (None, 30, 602)       2901640                                      \n",
      "____________________________________________________________________________________________________\n",
      "B_layer2 (LSTM)                  (None, 30, 602)       2901640                                      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 1204)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "A2_layer1 (LSTM)                 (None, 30, 1204)      11601744                                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 10003)     12053615                                     \n",
      "====================================================================================================\n",
      "Total params: 32,362,687\n",
      "Trainable params: 32,362,687\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############### One Hot ####################\n",
    "A1_input = Input(shape=(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "A1_layer1 = Bidirectional( LSTM(EMBED_DIM,name = \"A1_layer1\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT) )\n",
    "A1_layer2 = LSTM(EMBED_DIM*2,name = \"A1_layer2\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "A1_net = A1_layer2(A1_layer1(A1_input))\n",
    "\n",
    "B_input = Input(shape=(MAX_SENT_LENGTH,EMBED_DIM))\n",
    "\n",
    "B_layer1 = Bidirectional( LSTM(EMBED_DIM,name = \"B_layer1\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT) )\n",
    "B_layer2 = LSTM(EMBED_DIM*2,name = \"B_layer2\", return_sequences=True, dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "\n",
    "B_net = B_layer2(B_layer1(B_input))\n",
    "\n",
    "combined = concatenate([A1_net,B_net])\n",
    "A2_pred1 = LSTM(EMBED_DIM*4, name = \"A2_layer1\", return_sequences = True, dropout = LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n",
    "A2_pred2 = TimeDistributed(Dense(VOCAB_SIZE + 3, name = \"A2_layer2\", activation = 'softmax' ) )\n",
    "\n",
    "A2_net =A2_pred2(A2_pred1(combined))\n",
    "\n",
    "one_hot_chat_model = Model(inputs = [ A1_input,B_input ], outputs = [ A2_net ])\n",
    "one_hot_chat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint( 'one_hot_chat_net300dim.h5',verbose = 1,monitor = 'val_categorical_accuracy',save_best_only = True)\n",
    "early_stopping = EarlyStopping( monitor = 'val_categorical_accuracy',patience = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets = get_sets_of_data(size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word_frequencies data from disk\n"
     ]
    }
   ],
   "source": [
    "x = w2v.one_hot_vectorize(\"fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "adam = keras.optimizers.Adam(lr = 0.025)#default 0.001\n",
    "one_hot_chat_model.compile( optimizer=adam,loss = 'categorical_crossentropy',metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {
    "collapsed": false
   },

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 211s - loss: 7.8159 - categorical_accuracy: 0.2282 - val_loss: 4.2291 - val_categorical_accuracy: 0.4200\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 226s - loss: 4.3194 - categorical_accuracy: 0.4204 - val_loss: 4.1050 - val_categorical_accuracy: 0.4200\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 210s - loss: 3.8662 - categorical_accuracy: 0.4575 - val_loss: 3.8029 - val_categorical_accuracy: 0.4713\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 212s - loss: 3.7146 - categorical_accuracy: 0.4815 - val_loss: 3.6786 - val_categorical_accuracy: 0.4980\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 208s - loss: 3.5643 - categorical_accuracy: 0.5036 - val_loss: 3.5712 - val_categorical_accuracy: 0.5253\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 209s - loss: 3.5025 - categorical_accuracy: 0.5313 - val_loss: 3.2912 - val_categorical_accuracy: 0.5727\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 207s - loss: 3.4271 - categorical_accuracy: 0.5498 - val_loss: 3.2368 - val_categorical_accuracy: 0.5793\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 212s - loss: 3.3809 - categorical_accuracy: 0.5534 - val_loss: 3.1780 - val_categorical_accuracy: 0.5700\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 214s - loss: 3.3626 - categorical_accuracy: 0.5543 - val_loss: 3.1688 - val_categorical_accuracy: 0.5727\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 211s - loss: 3.3342 - categorical_accuracy: 0.5570 - val_loss: 3.1575 - val_categorical_accuracy: 0.5767\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 215s - loss: 3.4287 - categorical_accuracy: 0.5462 - val_loss: 3.5338 - val_categorical_accuracy: 0.5580\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 211s - loss: 3.4041 - categorical_accuracy: 0.5505 - val_loss: 3.5377 - val_categorical_accuracy: 0.5500\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 211s - loss: 3.3961 - categorical_accuracy: 0.5505 - val_loss: 3.5007 - val_categorical_accuracy: 0.5667\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 207s - loss: 3.4058 - categorical_accuracy: 0.5460 - val_loss: 3.5106 - val_categorical_accuracy: 0.5547\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 208s - loss: 3.3992 - categorical_accuracy: 0.5479 - val_loss: 3.5203 - val_categorical_accuracy: 0.5560\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 210s - loss: 3.3272 - categorical_accuracy: 0.5574 - val_loss: 3.0591 - val_categorical_accuracy: 0.5940\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 213s - loss: 3.3368 - categorical_accuracy: 0.5547 - val_loss: 3.0559 - val_categorical_accuracy: 0.5880\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 212s - loss: 3.3274 - categorical_accuracy: 0.5559 - val_loss: 3.0070 - val_categorical_accuracy: 0.5973\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 209s - loss: 3.3237 - categorical_accuracy: 0.5592 - val_loss: 3.0042 - val_categorical_accuracy: 0.5960\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 207s - loss: 3.3243 - categorical_accuracy: 0.5601 - val_loss: 3.0393 - val_categorical_accuracy: 0.6020\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 209s - loss: 3.3764 - categorical_accuracy: 0.5595 - val_loss: 3.5668 - val_categorical_accuracy: 0.5433\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 212s - loss: 3.3785 - categorical_accuracy: 0.5594 - val_loss: 3.5233 - val_categorical_accuracy: 0.5553\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 207s - loss: 3.3700 - categorical_accuracy: 0.5596 - val_loss: 3.5598 - val_categorical_accuracy: 0.5527\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 208s - loss: 3.3685 - categorical_accuracy: 0.5601 - val_loss: 3.6023 - val_categorical_accuracy: 0.5360\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 210s - loss: 3.4018 - categorical_accuracy: 0.5500 - val_loss: 3.5530 - val_categorical_accuracy: 0.5533\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 211s - loss: 3.2152 - categorical_accuracy: 0.5789 - val_loss: 4.0674 - val_categorical_accuracy: 0.4780\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 208s - loss: 3.1960 - categorical_accuracy: 0.5766 - val_loss: 4.0124 - val_categorical_accuracy: 0.4833\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 209s - loss: 3.1924 - categorical_accuracy: 0.5791 - val_loss: 4.0445 - val_categorical_accuracy: 0.4833\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 209s - loss: 3.1934 - categorical_accuracy: 0.5779 - val_loss: 4.0761 - val_categorical_accuracy: 0.4807\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 206s - loss: 3.1901 - categorical_accuracy: 0.5798 - val_loss: 4.0234 - val_categorical_accuracy: 0.4813\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 209s - loss: 3.4172 - categorical_accuracy: 0.5525 - val_loss: 3.4530 - val_categorical_accuracy: 0.5287\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 207s - loss: 3.4079 - categorical_accuracy: 0.5503 - val_loss: 3.3416 - val_categorical_accuracy: 0.5560\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 205s - loss: 3.3893 - categorical_accuracy: 0.5582 - val_loss: 3.3639 - val_categorical_accuracy: 0.5580\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 211s - loss: 3.4891 - categorical_accuracy: 0.5315 - val_loss: 3.5151 - val_categorical_accuracy: 0.5073\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 217s - loss: 3.4871 - categorical_accuracy: 0.5265 - val_loss: 3.4033 - val_categorical_accuracy: 0.5553\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 230s - loss: 3.3321 - categorical_accuracy: 0.5741 - val_loss: 3.2467 - val_categorical_accuracy: 0.5840\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 233s - loss: 3.2878 - categorical_accuracy: 0.5785 - val_loss: 3.1889 - val_categorical_accuracy: 0.5820\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 249s - loss: 3.2759 - categorical_accuracy: 0.5757 - val_loss: 3.1884 - val_categorical_accuracy: 0.5793\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 257s - loss: 3.2667 - categorical_accuracy: 0.5774 - val_loss: 3.1803 - val_categorical_accuracy: 0.5840\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 273s - loss: 3.2801 - categorical_accuracy: 0.5781 - val_loss: 3.1827 - val_categorical_accuracy: 0.5827\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 284s - loss: 3.3224 - categorical_accuracy: 0.5640 - val_loss: 3.4528 - val_categorical_accuracy: 0.5513\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 286s - loss: 3.3216 - categorical_accuracy: 0.5633 - val_loss: 3.4178 - val_categorical_accuracy: 0.5567\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 286s - loss: 3.3261 - categorical_accuracy: 0.5628 - val_loss: 3.4885 - val_categorical_accuracy: 0.5527\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 280s - loss: 3.3267 - categorical_accuracy: 0.5631 - val_loss: 3.3792 - val_categorical_accuracy: 0.5587\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 274s - loss: 3.3202 - categorical_accuracy: 0.5646 - val_loss: 3.3605 - val_categorical_accuracy: 0.5620\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 279s - loss: 3.3130 - categorical_accuracy: 0.5665 - val_loss: 3.4055 - val_categorical_accuracy: 0.5480\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 272s - loss: 3.2945 - categorical_accuracy: 0.5695 - val_loss: 3.4432 - val_categorical_accuracy: 0.5513\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 268s - loss: 3.2946 - categorical_accuracy: 0.5712 - val_loss: 3.3855 - val_categorical_accuracy: 0.5467\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 264s - loss: 3.2873 - categorical_accuracy: 0.5716 - val_loss: 3.3821 - val_categorical_accuracy: 0.5487\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950/950 [==============================] - 263s - loss: 3.2985 - categorical_accuracy: 0.5701 - val_loss: 3.4155 - val_categorical_accuracy: 0.5513\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 263s - loss: 3.4623 - categorical_accuracy: 0.5594 - val_loss: 3.5526 - val_categorical_accuracy: 0.5493\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 261s - loss: 3.4437 - categorical_accuracy: 0.5618 - val_loss: 3.5797 - val_categorical_accuracy: 0.5387\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 262s - loss: 3.4467 - categorical_accuracy: 0.5615 - val_loss: 3.5541 - val_categorical_accuracy: 0.5567\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 264s - loss: 3.4311 - categorical_accuracy: 0.5639 - val_loss: 3.5625 - val_categorical_accuracy: 0.5580\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 268s - loss: 3.4369 - categorical_accuracy: 0.5615 - val_loss: 3.5278 - val_categorical_accuracy: 0.5540\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 261s - loss: 3.4321 - categorical_accuracy: 0.5607 - val_loss: 3.4969 - val_categorical_accuracy: 0.5773\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 263s - loss: 3.4475 - categorical_accuracy: 0.5577 - val_loss: 3.5216 - val_categorical_accuracy: 0.5620\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 261s - loss: 3.4222 - categorical_accuracy: 0.5589 - val_loss: 3.5262 - val_categorical_accuracy: 0.5760\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 263s - loss: 3.4255 - categorical_accuracy: 0.5591 - val_loss: 3.5536 - val_categorical_accuracy: 0.5767\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 267s - loss: 3.4075 - categorical_accuracy: 0.5624 - val_loss: 3.5485 - val_categorical_accuracy: 0.5773\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 265s - loss: 3.2201 - categorical_accuracy: 0.5783 - val_loss: 3.2735 - val_categorical_accuracy: 0.6100\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 261s - loss: 3.2149 - categorical_accuracy: 0.5801 - val_loss: 3.3158 - val_categorical_accuracy: 0.6073\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 264s - loss: 3.2006 - categorical_accuracy: 0.5768 - val_loss: 3.2622 - val_categorical_accuracy: 0.5980\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 263s - loss: 3.2080 - categorical_accuracy: 0.5772 - val_loss: 3.3159 - val_categorical_accuracy: 0.5927\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 263s - loss: 3.2471 - categorical_accuracy: 0.5760 - val_loss: 3.3076 - val_categorical_accuracy: 0.6093\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 261s - loss: 3.5262 - categorical_accuracy: 0.5449 - val_loss: 3.1526 - val_categorical_accuracy: 0.6013\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 262s - loss: 3.5171 - categorical_accuracy: 0.5467 - val_loss: 3.1500 - val_categorical_accuracy: 0.6007\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 262s - loss: 3.4905 - categorical_accuracy: 0.5468 - val_loss: 3.1165 - val_categorical_accuracy: 0.5940\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 265s - loss: 3.4724 - categorical_accuracy: 0.5472 - val_loss: 3.1194 - val_categorical_accuracy: 0.5993\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 264s - loss: 3.4685 - categorical_accuracy: 0.5481 - val_loss: 3.1118 - val_categorical_accuracy: 0.5807\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 265s - loss: 3.3910 - categorical_accuracy: 0.5608 - val_loss: 3.4033 - val_categorical_accuracy: 0.5487\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 261s - loss: 3.3916 - categorical_accuracy: 0.5650 - val_loss: 3.3820 - val_categorical_accuracy: 0.5487\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 268s - loss: 3.3874 - categorical_accuracy: 0.5593 - val_loss: 3.3691 - val_categorical_accuracy: 0.5493\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 260s - loss: 3.3885 - categorical_accuracy: 0.5642 - val_loss: 3.3476 - val_categorical_accuracy: 0.5500\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 260s - loss: 3.3725 - categorical_accuracy: 0.5656 - val_loss: 3.3728 - val_categorical_accuracy: 0.5460\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 260s - loss: 3.4397 - categorical_accuracy: 0.5497 - val_loss: 2.7891 - val_categorical_accuracy: 0.6440\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 266s - loss: 3.4397 - categorical_accuracy: 0.5510 - val_loss: 2.7674 - val_categorical_accuracy: 0.6393\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 263s - loss: 3.4155 - categorical_accuracy: 0.5542 - val_loss: 2.8237 - val_categorical_accuracy: 0.6487\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 267s - loss: 3.4072 - categorical_accuracy: 0.5553 - val_loss: 2.7870 - val_categorical_accuracy: 0.6507\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 264s - loss: 3.4227 - categorical_accuracy: 0.5527 - val_loss: 2.7869 - val_categorical_accuracy: 0.6473\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 265s - loss: 3.3389 - categorical_accuracy: 0.5717 - val_loss: 3.6916 - val_categorical_accuracy: 0.5313\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 262s - loss: 3.3219 - categorical_accuracy: 0.5773 - val_loss: 3.7038 - val_categorical_accuracy: 0.5367\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 263s - loss: 3.3540 - categorical_accuracy: 0.5668 - val_loss: 3.7560 - val_categorical_accuracy: 0.5347\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 263s - loss: 3.3589 - categorical_accuracy: 0.5698 - val_loss: 3.7412 - val_categorical_accuracy: 0.5213\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 263s - loss: 3.3353 - categorical_accuracy: 0.5758 - val_loss: 3.7743 - val_categorical_accuracy: 0.5360\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 266s - loss: 3.4381 - categorical_accuracy: 0.5595 - val_loss: 3.2722 - val_categorical_accuracy: 0.5593\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 265s - loss: 3.4317 - categorical_accuracy: 0.5575 - val_loss: 3.2826 - val_categorical_accuracy: 0.5520\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 263s - loss: 3.4228 - categorical_accuracy: 0.5596 - val_loss: 3.2570 - val_categorical_accuracy: 0.5640\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 266s - loss: 3.4228 - categorical_accuracy: 0.5606 - val_loss: 3.2772 - val_categorical_accuracy: 0.5560\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 265s - loss: 3.4493 - categorical_accuracy: 0.5566 - val_loss: 3.2950 - val_categorical_accuracy: 0.5593\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 263s - loss: 3.5454 - categorical_accuracy: 0.5521 - val_loss: 3.1834 - val_categorical_accuracy: 0.6013\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 263s - loss: 3.5534 - categorical_accuracy: 0.5546 - val_loss: 3.2210 - val_categorical_accuracy: 0.5947\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 260s - loss: 3.5365 - categorical_accuracy: 0.5542 - val_loss: 3.1791 - val_categorical_accuracy: 0.6053\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 263s - loss: 3.5411 - categorical_accuracy: 0.5560 - val_loss: 3.1652 - val_categorical_accuracy: 0.6047\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 263s - loss: 3.5268 - categorical_accuracy: 0.5591 - val_loss: 3.2153 - val_categorical_accuracy: 0.5653\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 265s - loss: 3.3341 - categorical_accuracy: 0.5684 - val_loss: 3.3421 - val_categorical_accuracy: 0.5467\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 265s - loss: 3.3274 - categorical_accuracy: 0.5743 - val_loss: 3.3012 - val_categorical_accuracy: 0.5807\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 266s - loss: 3.3245 - categorical_accuracy: 0.5721 - val_loss: 3.3274 - val_categorical_accuracy: 0.5593\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 265s - loss: 3.3096 - categorical_accuracy: 0.5711 - val_loss: 3.3022 - val_categorical_accuracy: 0.5573\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 258s - loss: 3.3094 - categorical_accuracy: 0.5743 - val_loss: 3.3298 - val_categorical_accuracy: 0.5627\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 260s - loss: 3.3054 - categorical_accuracy: 0.5735 - val_loss: 2.8436 - val_categorical_accuracy: 0.6140\n",
      "Epoch 2/5\n",
      "800/950 [========================>.....] - ETA: 41s - loss: 3.2565 - categorical_accuracy: 0.5795"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ff5e18fb9ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           callbacks = [] )#checkpointer,early_stopping] )\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mone_hot_chat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'one_hot_chat_net300dim.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#one_hot_chat_model.load_weights('one_hot_chat_net_300dim.h5',by_name = True)\n",
    "i = 0\n",
    "for a1,b,a2 in sets:\n",
    "    A1_train,B_train,A2_train = w2v.get_training_data_one_hot_out(a1,b,a2)\n",
    "   \n",
    "    cat_a2 = []\n",
    "    for sent in A2_train:\n",
    "        cat_a2.append(to_categorical(sent,num_classes = VOCAB_SIZE + 3).astype('int8'))\n",
    "    cat_a2 = np.array(cat_a2)\n",
    "    one_hot_chat_model.fit([A1_train,B_train], cat_a2,\n",
    "          batch_size=50, epochs=5,\n",
    "          validation_split = 0.05,\n",
    "          callbacks = [] )#checkpointer,early_stopping] )\n",
    "    one_hot_chat_model.save('one_hot_chat_net300dim.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_vectorize( sentence, pad_length = -1, word_freqs = None ):\n",
    "    if word_freqs is None:\n",
    "        word_freqs = np.load('words_in_order_of_freq.npy')\n",
    "    \n",
    "    sentence = split_sentence( sentence )\n",
    "    words = sentence.split(\" \")\n",
    "    vectorized_sentence = []\n",
    "    \n",
    "    for word in words:\n",
    "        lower_word = word.lower()\n",
    "        number = word_freqs.index(lower_word)\n",
    "        if number > VOCAB_SIZE:\n",
    "            number = UNK\n",
    "        vectorized_sentence.append( number )\n",
    "\n",
    "    if( pad_length != -1 ):\n",
    "        while( len(vectorized_sentence) < pad_length ):\n",
    "            vectorized_sentence.append(NULL)\n",
    "    \n",
    "    return np.array(vectorized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(np.array([UNK]),num_classes = VOCAB_SIZE + 2 )["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10003)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(A2_train[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 10003)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2_train.shape"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 31,

   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",

   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freqs = np.load('words_in_order_of_freq.npy') \n",
    "\n",
    "def get_random_test_sentence():\n",
    "    index = random.randint(0,len(A1)-1)\n",
    "    print A1[index]\n",
    "    print B[index]\n",
    "    print A2[index]\n",
    "    return( w2v.get_training_data_one_hot_out(A1[index:index+1],B[index:index+1],A2[index:index+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 8 1 @ ME ? ! No , uh , I'm just a transvestite .\n",
      "1 9 1 @ Isn't that the same thing ?\n",
      "2 8 1 @ No , no ! I like girls . So how 'bout Friday ?\n"
     ]
    }
   ],
   "source": [
    "A1_test,B_test,A2_test = get_random_test_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = one_hot_chat_model.predict([A1_test,B_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prizes prizes prizes prizes prizes prizes prizes prizes prizes prizes prizes prizes prizes prizes prizes prizes poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom poolroom\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_words = []\n",
    "for word_vec in predict[0]:\n",
    "    #print(word_vec )\n",
    "    pred_words.append( word_freqs[np.where(word_vec==max(word_vec))[0][0]] )\n",
    "print( \" \".join(pred_words))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    one_hot_chat_model.save('one_hot_chat_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
